"""LoRA fine-tune chinese-roberta-wwm-ext for 3-class content filtering."""
from pathlib import Path
import json
import argparse
import torch

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  # type: ignore
from datasets import Dataset
from peft import LoraConfig, get_peft_model  # type: ignore
from sklearn.metrics import accuracy_score


def load_jsonl(path):
    """åŠ è½½ JSONL æ–‡ä»¶ï¼Œæ¯è¡Œä¸ºä¸€ä¸ªæ ·æœ¬ï¼Œè¿”å›å­—å…¸ç”Ÿæˆå™¨"""
    # æ ‡ç­¾æ˜ å°„ï¼šå­—ç¬¦ä¸²æ ‡ç­¾è½¬æ¢ä¸ºæ•´æ•°
    label_map = {"normal": 0, "violation": 1, "suspicious": 2}

    for line in Path(path).read_text(encoding="utf-8").splitlines():
        obj = json.loads(line)
        # è·å–æ ‡ç­¾å­—ç¬¦ä¸²å¹¶è½¬æ¢ä¸ºæ•´æ•°
        label_str = obj.get("label", "normal")
        label = label_map.get(label_str, 0)  # é»˜è®¤ä¸ºnormal(0)
        yield {"text": obj["text"], "label": label}


def make_ds(path):
    """å°† JSONL æ–‡ä»¶è½¬æ¢ä¸º Huggingface Datasets æ ¼å¼"""
    return Dataset.from_list(list(load_jsonl(path)))


# è§£æå‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser()
parser.add_argument("--train", default="data/annotations/train.jsonl", help="è®­ç»ƒé›†è·¯å¾„")
parser.add_argument("--eval",  default="data/annotations/validation.jsonl", help="éªŒè¯é›†è·¯å¾„")
parser.add_argument("--out",   default="models/lora_roberta_ckpt", help="æ¨¡å‹è¾“å‡ºç›®å½•")
parser.add_argument("--quick", action="store_true", help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šå°batch sizeå’Œå°‘é‡epoch")
args = parser.parse_args()

# è‡ªåŠ¨é€‰æ‹© base_model è·¯å¾„
local_base_model = Path("chinese-roberta-wwm-ext")
if local_base_model.exists() and (local_base_model / "config.json").exists():
    model_name = str(local_base_model)
    print(f"[INFO] ä½¿ç”¨æœ¬åœ°åŸºåº§æ¨¡å‹: {model_name}")
else:
    model_name = "hfl/chinese-roberta-wwm-ext"
    print(f"[INFO] ä½¿ç”¨ HuggingFace Hub æ¨¡å‹: {model_name}")

model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=3,
    torch_dtype=torch.float32,  # æ˜ç¡®æŒ‡å®šæ•°æ®ç±»å‹
    low_cpu_mem_usage=True      # ä½å†…å­˜ä½¿ç”¨æ¨¡å¼
)

# æ³¨å…¥ LoRA
lora_cfg = LoraConfig(r=16, lora_alpha=32, target_modules=["query", "value"])  # type: ignore
model = get_peft_model(model, lora_cfg)

# é‡åŒ–è‡³ 8-bit å¹¶è½¬ä¸º float32
model = model.to(torch.float32)

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)


def tokenize(batch):
    """æ‰¹é‡åˆ†è¯ï¼Œæœ€å¤§é•¿åº¦128"""
    return tokenizer(batch["text"], truncation=True, padding="max_length", max_length=128)


# åŠ è½½å¹¶åˆ†è¯è®­ç»ƒ/éªŒè¯é›†
train_ds = make_ds(args.train).map(tokenize, batched=True)
val_ds = make_ds(args.eval).map(tokenize, batched=True)


def compute_metrics(eval_pred):
    try:
        logits, labels = eval_pred
        preds = logits.argmax(-1)
        acc = accuracy_score(labels, preds)
        return {"accuracy": acc, "eval_accuracy": acc}
    except Exception:
        # ä¸‡ä¸€è¯„ä¼°å‡ºé”™ï¼Œè¿”å›é»˜è®¤å€¼ï¼Œé¿å… KeyError
        return {"accuracy": 0.0, "eval_accuracy": 0.0}


# æ£€æµ‹æ˜¯å¦æœ‰ GPU æ”¯æŒ fp16
fp16_enabled = torch.cuda.is_available()  # åªæœ‰ CUDA GPU æ‰æ”¯æŒ fp16

# è®­ç»ƒå‚æ•°é…ç½®
if args.quick:
    # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šå°batch sizeï¼Œå°‘é‡epoch
    print("ğŸ”§ å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨å°batch sizeå’Œå°‘é‡epoch")
    train_args = TrainingArguments(
        output_dir=args.out,
        per_device_train_batch_size=8,   # å‡å°batch size
        per_device_eval_batch_size=8,
        num_train_epochs=1,              # åªè®­ç»ƒ1ä¸ªepoch
        learning_rate=2e-5,
        evaluation_strategy="no",        # å¿«é€Ÿæ¨¡å¼ä¸‹ä¸è¿›è¡Œè¯„ä¼°
        save_strategy="epoch",           # æ¯ä¸ªepochä¿å­˜
        fp16=fp16_enabled,               # æ ¹æ®ç¯å¢ƒè‡ªåŠ¨è®¾ç½®
        logging_steps=10,                # æ›´é¢‘ç¹çš„æ—¥å¿—
        save_total_limit=1,
        load_best_model_at_end=False,    # å¿«é€Ÿæ¨¡å¼ä¸‹ä¸åŠ è½½æœ€ä½³æ¨¡å‹
    )
else:
    # æ­£å¸¸è®­ç»ƒæ¨¡å¼
    train_args = TrainingArguments(
        output_dir=args.out,
        per_device_train_batch_size=32,
        per_device_eval_batch_size=32,
        num_train_epochs=3,
        learning_rate=2e-5,
        evaluation_strategy="epoch",
        save_strategy="epoch",           # æ¯ä¸ªepochä¿å­˜
        fp16=fp16_enabled,               # æ ¹æ®ç¯å¢ƒè‡ªåŠ¨è®¾ç½®
        logging_steps=50,
        save_total_limit=1,
        load_best_model_at_end=True,
        metric_for_best_model="eval_accuracy",  # è¿™é‡Œå¼ºåˆ¶ç”¨ eval_accuracy
    )


class SafeTrainer(Trainer):
    def _save_checkpoint(self, model, trial, metrics=None):
        # å¦‚æœ metrics é‡Œæ²¡æœ‰ metric_for_best_modelï¼Œè¡¥ä¸€ä¸ªé»˜è®¤å€¼
        if metrics is not None and self.args.metric_for_best_model is not None:
            if self.args.metric_for_best_model not in metrics:
                metrics[self.args.metric_for_best_model] = 0.0
        super()._save_checkpoint(model, trial, metrics)


# æ„å»º SafeTrainer å¹¶è®­ç»ƒ
trainer = SafeTrainer(model=model, args=train_args, train_dataset=train_ds, eval_dataset=val_ds,
                      compute_metrics=compute_metrics)
trainer.train()
trainer.save_model(args.out)
print("LoRA checkpoint saved â†’", args.out)
