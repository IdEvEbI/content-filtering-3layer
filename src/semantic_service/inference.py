"""LoRA model inference script for content filtering."""
import argparse
import torch
from pathlib import Path
from typing import List, Dict, Any

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import PeftModel  # type: ignore


class ContentFilterInference:
    """å†…å®¹è¿‡æ»¤æ¨ç†ç±»"""

    def __init__(self, model_path: str, base_model_name: str = ''):
        """
        åˆå§‹åŒ–æ¨ç†æ¨¡å‹
        Args:
            model_path: LoRAæ¨¡å‹è·¯å¾„
            base_model_name: åŸºç¡€æ¨¡å‹åç§°ï¼ˆå¯ä¸º''ï¼Œè‡ªåŠ¨é€‰æ‹©ï¼‰
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.label_map = {0: "normal", 1: "violation", 2: "suspicious"}

        # è‡ªåŠ¨é€‰æ‹© base_model è·¯å¾„
        if not base_model_name:
            local_base_model = Path("chinese-roberta-wwm-ext")
            if local_base_model.exists() and (local_base_model / "config.json").exists():
                base_model_name = str(local_base_model)
                print(f"[INFO] ä½¿ç”¨æœ¬åœ°åŸºåº§æ¨¡å‹: {base_model_name}")
            else:
                base_model_name = "hfl/chinese-roberta-wwm-ext"
                print(f"[INFO] ä½¿ç”¨ HuggingFace Hub æ¨¡å‹: {base_model_name}")
        else:
            print(f"åŠ è½½åº•åº§æ¨¡å‹è·¯å¾„: {base_model_name}")

        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=False)

        # åŠ è½½åŸºç¡€æ¨¡å‹
        self.base_model = AutoModelForSequenceClassification.from_pretrained(
            base_model_name,
            num_labels=3,
            torch_dtype=torch.float32,
            low_cpu_mem_usage=True
        )

        # åŠ è½½LoRAæƒé‡
        self.model = PeftModel.from_pretrained(self.base_model, model_path)
        self.model.to(self.device)
        self.model.eval()

        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {self.device}")

    def predict(self, text: str) -> Dict[str, Any]:
        """
        å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œé¢„æµ‹
        Args:
            text: è¾“å…¥æ–‡æœ¬
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        # åˆ†è¯
        inputs = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=128,
            return_tensors="pt"
        ).to(self.device)

        # æ¨ç†
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=-1)

            # è·å–é¢„æµ‹ç»“æœ
            predicted_class = int(torch.argmax(logits, dim=-1).item())
            confidence = float(probabilities[0][predicted_class].item())

            # è·å–æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
            all_probs = probabilities[0].tolist()

        return {
            "text": text,
            "predicted_label": self.label_map[predicted_class],
            "predicted_class": predicted_class,
            "confidence": confidence,
            "probabilities": {
                self.label_map[i]: prob for i, prob in enumerate(all_probs)
            }
        }

    def batch_predict(self, texts: List[str]) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡é¢„æµ‹
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        results = []
        for text in texts:
            results.append(self.predict(text))
        return results


def load_test_samples() -> List[str]:
    """åŠ è½½æµ‹è¯•æ ·æœ¬"""
    return [
        "è¿™æ˜¯ä¸€ä¸ªæ­£å¸¸çš„å¸–å­å†…å®¹ï¼Œæ²¡æœ‰ä»»ä½•è¿è§„ä¿¡æ¯ã€‚",
        "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œå¤§å®¶ä¸€èµ·æ¥è®¨è®ºä¸€ä¸‹ã€‚",
        "è¿™ä¸ªäº§å“çœŸçš„å¾ˆä¸é”™ï¼Œæ¨èç»™å¤§å®¶ä½¿ç”¨ã€‚",
        "æˆ‘æƒ³äº†è§£ä¸€ä¸‹è¿™ä¸ªæŠ€æœ¯é—®é¢˜ï¼Œæœ‰è°å¯ä»¥å¸®å¿™è§£ç­”å—ï¼Ÿ",
        "è¿™ä¸ªè®ºå›çš„ç®¡ç†å‘˜å¾ˆè´Ÿè´£ä»»ï¼Œç»´æŠ¤å¾—å¾ˆå¥½ã€‚",
        "æˆ‘è§‰å¾—è¿™ä¸ªè§‚ç‚¹å¾ˆæœ‰é“ç†ï¼Œå€¼å¾—æ·±å…¥æ€è€ƒã€‚",
        "è¿™ä¸ªæ´»åŠ¨å¾ˆæœ‰æ„æ€ï¼Œæˆ‘ä¹Ÿè¦å‚åŠ ã€‚",
        "è°¢è°¢å¤§å®¶çš„å¸®åŠ©ï¼Œé—®é¢˜å·²ç»è§£å†³äº†ã€‚",
        "è¿™ä¸ªè½¯ä»¶çš„åŠŸèƒ½å¾ˆå¼ºå¤§ï¼Œä½¿ç”¨èµ·æ¥å¾ˆæ–¹ä¾¿ã€‚",
        "æˆ‘è§‰å¾—è¿™ä¸ªå»ºè®®å¾ˆå¥½ï¼Œå¯ä»¥è€ƒè™‘é‡‡çº³ã€‚"
    ]


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="LoRAæ¨¡å‹æ¨ç†æµ‹è¯•")
    parser.add_argument("--model", default="models/quick_test_ckpt", help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--base_model", default='', help="æœ¬åœ°åŸºç¡€æ¨¡å‹è·¯å¾„æˆ–åç§°ï¼ˆç•™ç©ºåˆ™è‡ªåŠ¨é€‰æ‹©ï¼‰")
    parser.add_argument("--text", help="å•æ¡æ–‡æœ¬æµ‹è¯•")
    parser.add_argument("--batch", action="store_true", help="æ‰¹é‡æµ‹è¯•æ¨¡å¼")
    args = parser.parse_args()

    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    model_path = Path(args.model)
    if not model_path.exists():
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return

    # åˆå§‹åŒ–æ¨ç†æ¨¡å‹
    try:
        base_model_arg = args.base_model if args.base_model else ''
        inference = ContentFilterInference(str(model_path), base_model_name=base_model_arg)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # å•æ¡æ–‡æœ¬æµ‹è¯•
    if args.text:
        result = inference.predict(args.text)
        print("\nğŸ” å•æ¡æ–‡æœ¬é¢„æµ‹ç»“æœ:")
        print(f"æ–‡æœ¬: {result['text']}")
        print(f"é¢„æµ‹æ ‡ç­¾: {result['predicted_label']} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")
        print("å„ç±»åˆ«æ¦‚ç‡:")
        for label, prob in result['probabilities'].items():
            print(f"  {label}: {prob:.3f}")
        return

    # æ‰¹é‡æµ‹è¯•
    if args.batch:
        test_samples = load_test_samples()
        results = inference.batch_predict(test_samples)

        print("\nğŸ” æ‰¹é‡æµ‹è¯•ç»“æœ:")
        print("-" * 80)
        for i, result in enumerate(results, 1):
            print(f"{i:2d}. æ–‡æœ¬: {result['text'][:50]}...")
            print(f"    é¢„æµ‹: {result['predicted_label']} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")
            print(f"    æ¦‚ç‡åˆ†å¸ƒ: normal={result['probabilities']['normal']:.3f}, "
                  f"violation={result['probabilities']['violation']:.3f}, "
                  f"suspicious={result['probabilities']['suspicious']:.3f}")
            print()
        return

    # äº¤äº’å¼æµ‹è¯•
    print("\nğŸ¯ äº¤äº’å¼æ¨ç†æµ‹è¯• (è¾“å…¥ 'quit' é€€å‡º)")
    print("-" * 50)

    while True:
        text = input("\nè¯·è¾“å…¥è¦æµ‹è¯•çš„æ–‡æœ¬: ").strip()
        if text.lower() in ['quit', 'exit', 'q']:
            break

        if not text:
            continue

        try:
            result = inference.predict(text)
            print("\nğŸ“Š é¢„æµ‹ç»“æœ:")
            print(f"  æ ‡ç­¾: {result['predicted_label']}")
            print(f"  ç½®ä¿¡åº¦: {result['confidence']:.3f}")
            print("  æ¦‚ç‡åˆ†å¸ƒ:")
            for label, prob in result['probabilities'].items():
                bar = "â–ˆ" * int(prob * 20)
                print(f"    {label:10s}: {prob:.3f} {bar}")
        except Exception as e:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")


if __name__ == "__main__":
    main()
