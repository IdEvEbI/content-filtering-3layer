# Phaseâ€¯B æ­¥éª¤â€¯1 â€” LoRAâ€‘BERT å¾®è°ƒè„šæœ¬

> æœ¬ç¬”è®°è®²è§£ **Phaseâ€¯B Â· Stepâ€¯1**ï¼šåŸºäº `hfl/chinese-roberta-wwm-ext` ä¸ LoRAï¼Œè®­ç»ƒä¸‰åˆ†ç±»è¯­ä¹‰æ¨¡å‹ï¼ˆ0â€¯åˆè§„ Â·â€¯1â€¯ç–‘ä¼¼ Â·â€¯2â€¯è¿è§„ï¼‰ã€‚è®­ç»ƒåœ¨é˜¿é‡Œäº‘ GPU å®ä¾‹å®Œæˆï¼Œéšåå¯¼å‡ºæƒé‡ä¾›æ¨ç†æœåŠ¡ä½¿ç”¨ã€‚

---

## 1â€¯å®æ“æ­¥éª¤

> å››å—å†…å®¹ï¼š**ä¾èµ–å®‰è£… â†’ è®­ç»ƒè„šæœ¬ â†’ äº‘ç«¯è®­ç»ƒæŒ‡å— â†’ æäº¤ PR**ã€‚

### 1â€‘1 ä¾èµ–å®‰è£…

```bash
# æ ¸å¿ƒä¾èµ–åŒ…
pip install transformers==4.41.0
pip install peft==0.10.0
pip install bitsandbytes==0.42.0
pip install accelerate==1.8.1
pip install datasets==3.6.0
pip install evaluate==0.4.1
pip install scikit-learn==1.7.0

# å¯é€‰ï¼šå¦‚æœé‡åˆ°ç½‘ç»œé—®é¢˜ï¼Œä½¿ç”¨å›½å†…é•œåƒ
# pip install -i https://pypi.tuna.tsinghua.edu.cn/simple/ transformers peft bitsandbytes accelerate datasets evaluate

# é”å®šä¾èµ–
pip freeze | grep -E "(transformers|peft|bitsandbytes|accelerate|datasets|evaluate|scikit-learn)" >> requirements.txt
```

**ä¾èµ–è¯´æ˜**ï¼š

* **transformers**ï¼šHugging Face æ¨¡å‹åº“ï¼Œæä¾›é¢„è®­ç»ƒæ¨¡å‹å’Œè®­ç»ƒæ¡†æ¶
* **peft**ï¼šParameter-Efficient Fine-Tuningï¼Œå®ç° LoRA ç­‰é«˜æ•ˆå¾®è°ƒæ–¹æ³•
* **bitsandbytes**ï¼š8-bit é‡åŒ–æ”¯æŒï¼Œæ˜¾è‘—å‡å°‘æ˜¾å­˜å ç”¨
* **accelerate**ï¼šåˆ†å¸ƒå¼è®­ç»ƒå’Œæ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
* **datasets**ï¼šé«˜æ•ˆçš„æ•°æ®é›†åŠ è½½å’Œå¤„ç†
* **evaluate**ï¼šæ¨¡å‹è¯„ä¼°æŒ‡æ ‡åº“ï¼ˆæ›¿ä»£å·²å¼ƒç”¨çš„ load_metricï¼‰
* **scikit-learn**ï¼šæœºå™¨å­¦ä¹ å·¥å…·åº“ï¼Œæä¾›åˆ†ç±»è¯„ä¼°æŒ‡æ ‡å’Œæ•°æ®å¤„ç†åŠŸèƒ½

> *æ˜¾å¡ â‰¥ T4 16 GB å¯ç›´æ¥ fp16ï¼›å¦‚ CPU è®­ç»ƒéœ€ç§»é™¤ `--fp16`ã€‚*

---

### 1â€‘2 è®­ç»ƒè„šæœ¬ï¼ˆ`src/semantic_service/train.py`ï¼‰

#### 1-2-1 ç¼–å†™è„šæœ¬

```python
"""LoRA fine-tune chinese-roberta-wwm-ext for 3-class content filtering."""
from pathlib import Path
import json
import argparse
import torch
import os

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  # type: ignore
from datasets import Dataset
import evaluate  # æ–°å¢ evaluate åº“ç”¨äºåŠ è½½ metric
from peft import LoraConfig, get_peft_model  # type: ignore


def load_jsonl(path):
    """åŠ è½½ JSONL æ–‡ä»¶ï¼Œæ¯è¡Œä¸ºä¸€ä¸ªæ ·æœ¬ï¼Œè¿”å›å­—å…¸ç”Ÿæˆå™¨"""
    # æ ‡ç­¾æ˜ å°„ï¼šå­—ç¬¦ä¸²æ ‡ç­¾è½¬æ¢ä¸ºæ•´æ•°
    label_map = {"normal": 0, "violation": 1, "suspicious": 2}

    for line in Path(path).read_text(encoding="utf-8").splitlines():
        obj = json.loads(line)
        # è·å–æ ‡ç­¾å­—ç¬¦ä¸²å¹¶è½¬æ¢ä¸ºæ•´æ•°
        label_str = obj.get("label", "normal")
        label = label_map.get(label_str, 0)  # é»˜è®¤ä¸ºnormal(0)
        yield {"text": obj["text"], "label": label}


def make_ds(path):
    """å°† JSONL æ–‡ä»¶è½¬æ¢ä¸º Huggingface Datasets æ ¼å¼"""
    return Dataset.from_list(list(load_jsonl(path)))


# è§£æå‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser()
parser.add_argument("--train", default="data/annotations/train.jsonl", help="è®­ç»ƒé›†è·¯å¾„")
parser.add_argument("--eval",  default="data/annotations/validation.jsonl", help="éªŒè¯é›†è·¯å¾„")
parser.add_argument("--out",   default="models/lora_roberta_ckpt", help="æ¨¡å‹è¾“å‡ºç›®å½•")
parser.add_argument("--quick", action="store_true", help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šå°batch sizeå’Œå°‘é‡epoch")
args = parser.parse_args()

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model_name = "hfl/chinese-roberta-wwm-ext"
cache_path = os.path.expanduser(
      "~/.cache/huggingface/hub/models--hfl--chinese-roberta-wwm-ext/snapshots/5c58d0b8ec1d9014354d691c538661bf00bfdb44"
    )
model = AutoModelForSequenceClassification.from_pretrained(
    cache_path,
    num_labels=3,
    torch_dtype=torch.float32,  # æ˜ç¡®æŒ‡å®šæ•°æ®ç±»å‹
    low_cpu_mem_usage=True      # ä½å†…å­˜ä½¿ç”¨æ¨¡å¼
)

# æ³¨å…¥ LoRA
lora_cfg = LoraConfig(r=16, lora_alpha=32, target_modules=["query", "value"])  # type: ignore
model = get_peft_model(model, lora_cfg)

# é‡åŒ–è‡³ 8-bit å¹¶è½¬ä¸º float16
model = model.to(torch.float32)

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(cache_path, use_fast=False)


def tokenize(batch):
    """æ‰¹é‡åˆ†è¯ï¼Œæœ€å¤§é•¿åº¦128"""
    return tokenizer(batch["text"], truncation=True, padding="max_length", max_length=128)


# åŠ è½½å¹¶åˆ†è¯è®­ç»ƒ/éªŒè¯é›†
train_ds = make_ds(args.train).map(tokenize, batched=True)
val_ds = make_ds(args.eval).map(tokenize, batched=True)

# åŠ è½½å‡†ç¡®ç‡è¯„ä¼°æŒ‡æ ‡ï¼ˆæ–°ç‰ˆ evaluate æ›¿æ¢ load_metricï¼‰
metric = evaluate.load("accuracy")


def compute_metrics(eval_pred):
    """è¯„ä¼°å‡½æ•°ï¼Œè¿”å›å‡†ç¡®ç‡"""
    logits, labels = eval_pred
    preds = logits.argmax(-1)
    result = metric.compute(predictions=preds, references=labels)
    return result or {}  # ç¡®ä¿è¿”å›å­—å…¸è€Œä¸æ˜¯ None


# æ£€æµ‹æ˜¯å¦æœ‰ GPU æ”¯æŒ fp16
fp16_enabled = torch.cuda.is_available()  # åªæœ‰ CUDA GPU æ‰æ”¯æŒ fp16

# è®­ç»ƒå‚æ•°é…ç½®
if args.quick:
    # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šå°batch sizeï¼Œå°‘é‡epoch
    print("ğŸ”§ å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨å°batch sizeå’Œå°‘é‡epoch")
    train_args = TrainingArguments(
        output_dir=args.out,
        per_device_train_batch_size=8,   # å‡å°batch size
        per_device_eval_batch_size=8,
        num_train_epochs=1,              # åªè®­ç»ƒ1ä¸ªepoch
        learning_rate=2e-5,
        evaluation_strategy="no",        # å¿«é€Ÿæ¨¡å¼ä¸‹ä¸è¿›è¡Œè¯„ä¼°
        save_strategy="epoch",           # æ¯ä¸ªepochä¿å­˜
        fp16=fp16_enabled,               # æ ¹æ®ç¯å¢ƒè‡ªåŠ¨è®¾ç½®
        logging_steps=10,                # æ›´é¢‘ç¹çš„æ—¥å¿—
        save_total_limit=1,
        load_best_model_at_end=False,    # å¿«é€Ÿæ¨¡å¼ä¸‹ä¸åŠ è½½æœ€ä½³æ¨¡å‹
    )
else:
    # æ­£å¸¸è®­ç»ƒæ¨¡å¼
    train_args = TrainingArguments(
        output_dir=args.out,
        per_device_train_batch_size=32,
        per_device_eval_batch_size=32,
        num_train_epochs=3,
        learning_rate=2e-5,
        evaluation_strategy="epoch",
        save_strategy="epoch",           # æ¯ä¸ªepochä¿å­˜
        fp16=fp16_enabled,               # æ ¹æ®ç¯å¢ƒè‡ªåŠ¨è®¾ç½®
        logging_steps=50,
        save_total_limit=1,
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",  # ä½¿ç”¨å‡†ç¡®ç‡ä½œä¸ºæœ€ä½³æ¨¡å‹æŒ‡æ ‡
    )

# æ„å»º Trainer å¹¶è®­ç»ƒ
trainer = Trainer(model=model, args=train_args, train_dataset=train_ds, eval_dataset=val_ds,
                  compute_metrics=compute_metrics)
trainer.train()
trainer.save_model(args.out)
print("LoRA checkpoint saved â†’", args.out)

```

> **è„šæœ¬ç‰¹ç‚¹**ï¼š
>
> * LoRA r16 + float32 â†’ æ˜¾å­˜å ç”¨ â‰ˆ 6â€¯GB
> * `tokenize` 128 tokensï¼Œé€‚é…è®ºå›çŸ­æ–‡æœ¬
> * `load_best_model_at_end` è‡ªåŠ¨ä¿å­˜æœ€ä¼˜ epoch
> * æ”¯æŒå¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼Œä¾¿äºæœ¬åœ°éªŒè¯

### 1-2-2 å¿«é€Ÿæµ‹è¯•

```bash
# å¿«é€Ÿæµ‹è¯•ï¼š1ä¸ªepochï¼Œbatch size=8
python src/semantic_service/train.py --quick --out models/quick_test_ckpt
```

**ç‰¹ç‚¹**ï¼š

* ä½¿ç”¨å®Œæ•´æ•°æ®é›†ï¼Œä½†è®­ç»ƒå‚æ•°ä¼˜åŒ–ä¸ºå¿«é€Ÿæ¨¡å¼
* batch size: 8ï¼ˆæ­£å¸¸32ï¼‰
* epochs: 1ï¼ˆæ­£å¸¸3ï¼‰
* é¢„è®¡æ—¶é—´ï¼š5-10åˆ†é’Ÿ
* é€‚åˆéªŒè¯ç¯å¢ƒå’Œè„šæœ¬é…ç½®

### 1-2-3 æœ¬åœ°æ¨¡å‹æ•ˆæœéªŒè¯

è®­ç»ƒå®Œæˆåï¼Œå¯ç›´æ¥ç”¨æ¨ç†è„šæœ¬å¯¹æ¨¡å‹æ•ˆæœè¿›è¡Œæœ¬åœ°éªŒè¯ã€‚

```bash
# å•æ¡æ–‡æœ¬æ¨ç†
python src/semantic_service/inference.py --model models/quick_test_ckpt --text "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"

# æ‰¹é‡æ¨ç†æµ‹è¯•
python src/semantic_service/inference.py --model models/quick_test_ckpt --batch
```

**è¯´æ˜**ï¼š

* æ¨ç†è„šæœ¬æ”¯æŒå•æ¡æ–‡æœ¬ã€æ‰¹é‡å’Œäº¤äº’å¼ä¸‰ç§æ¨¡å¼
* è¾“å‡ºåŒ…æ‹¬é¢„æµ‹æ ‡ç­¾ã€ç½®ä¿¡åº¦å’Œå„ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒ
* å¯ç”¨äºå¿«é€Ÿè¯„ä¼°æ¨¡å‹è®­ç»ƒæ•ˆæœ

---

### 1â€‘3 äº‘ç«¯è®­ç»ƒæŒ‡å—ï¼ˆé˜¿é‡Œäº‘ A10 24â€¯GB å®ä¾‹ç¤ºä¾‹ï¼‰

è¯¦ç»†çš„éƒ¨ç½²ä¸è®­ç»ƒæµç¨‹è¯·å‚è€ƒï¼š[docs/note/aliyun-lora-train-deploy.md](../note/aliyun-lora-train-deploy.md)

**æ ¸å¿ƒæµç¨‹æ‘˜è¦ï¼š**

1. å¼€é€š GPU å®ä¾‹ï¼ˆæ¨è A10 24GBï¼‰
2. SSH ç™»å½•ï¼Œå‡†å¤‡ Python ç¯å¢ƒä¸ä¾èµ–
3. ä¸Šä¼ æ•°æ®ä¸ä»£ç 
4. è¿è¡Œè®­ç»ƒè„šæœ¬
5. ä¸‹è½½æ¨¡å‹æƒé‡ï¼Œå…³åœå®ä¾‹

> è¯¦ç»†å‘½ä»¤ã€ä¾èµ–åŠ é€Ÿã€å¸¸è§é—®é¢˜æ’æŸ¥ç­‰è¯·è§éƒ¨ç½²æŒ‡å—æ–‡æ¡£ã€‚

---

### 1â€‘4 æäº¤ PR

```bash
git switch -c lora-train

git add src/semantic_service/train.py requirements.txt models/.gitkeep docs/phase-b/ali-gpu-setup.md

git commit -m "feat: add lora finetune script & cloud guide"

git push -u origin lora-train
# GitHub: åˆ›å»º PR âœ base: dev â† compare: lora-train
# CIï¼ˆCPUï¼‰å¯ Skipping heavy testsï¼›ä»£ç  lint é€šè¿‡ âœ Merge
```

---

## 2â€¯é™„åŠ è¯´æ˜

* **batch\_size è°ƒæ•´**ï¼šå¦‚ GPU ä»… 16â€¯GBï¼Œå¯é™åˆ° 16 å¹¶ç”¨æ¢¯åº¦ç´¯ç§¯ã€‚
* **æ ‡ç­¾ä¸å¹³è¡¡**ï¼šåˆæœŸè¿è§„æ ·æœ¬å°‘ï¼ŒTrainer è‡ªåŠ¨ä½¿ç”¨åŠ æƒ lossï¼ˆ`class_weight` å¯åç»­è¡¥ï¼‰ã€‚
* **æ´»è·ƒå­¦ä¹ å¾ªç¯**ï¼šè®­ç»ƒåç”¨æ¨¡å‹æ‰“åˆ†å…¨éƒ¨ TSVï¼Œé€‰ç½®ä¿¡åº¦ 0.4â€“0.6 å†äººå·¥æ ‡æ³¨ã€‚

---

## æ€»ç»“

1. LoRAâ€‘BERT ä¸‰åˆ†ç±»è®­ç»ƒè„šæœ¬å°±ç»ªï¼Œæ˜¾å­˜å ç”¨ä½ã€è®­ç»ƒ 3â€¯epoch â‰ˆ 30â€¯minï¼ˆA10ï¼‰ã€‚
2. é˜¿é‡Œäº‘ GPU ä½¿ç”¨æ–‡æ¡£æä¾›ä»å¼€æœºåˆ°å…³æœºå®Œæ•´ CLIã€‚
3. PR åˆå¹¶åå³è¿›å…¥ **Stepâ€¯B2**ï¼šé¦–è½®è®­ç»ƒ & è¯„ä¼°æŠ¥å‘Šã€‚

> **ä¸€å¥è¯æ€»ç»“**ï¼šè½»é‡ LoRA å¾®è°ƒï¼Œè®©è®ºå›è¯­ä¹‰è¿‡æ»¤åœ¨æœ¬åœ° GPU æˆæœ¬å†…å¯è¡Œã€‚

---

## å»ºè®®æ–‡æ¡£å

`phase-b-step1-lora-train`

## å»ºè®®ä¿å­˜ç›®å½•è·¯å¾„

`docs/phase-b`

## æ¨è Commit Message

`feat: add lora finetune script & cloud guide`
